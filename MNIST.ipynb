{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Loading input data\n",
    "We will use placeholder for this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#download mnist dataset\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "#create a placeholder with two dimension\n",
    "#the first one is yet unknown the second one is the number of features in mnist i.e. 28x28 image\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "\n",
    "#create a placeholder for the labels\n",
    "y = tf.placeholder(tf.int64, [None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Model Definition\n",
    "Softmax Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define variables\n",
    "W = tf.Variable(tf.zeros([784,10])) #784 inputs, 10 nodes as output => 1 vs all classification\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#model prediction\n",
    "prediction_logits = tf.matmul(x,W)+b\n",
    "prediction = tf.nn.softmax(prediction_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = tf.get_variable(\"w_1\",shape=[784,50],initializer=tf.contrib.layers.xavier_initializer())  #784 inputs, 50 nodes as output => 50 hidden units\n",
    "b_1 = tf.get_variable(\"b_1\",shape=[50],initializer=tf.random_uniform_initializer(-0.5,0.5))\n",
    "\n",
    "#fully connected layer 1\n",
    "fc1 = tf.nn.relu(tf.matmul(x,W_1)+b_1)  #relu activation\n",
    "\n",
    "W_2 = tf.get_variable(\"w_2\",shape=[50,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_2 = tf.get_variable(\"b_2\",shape=[10],initializer=tf.random_uniform_initializer(-0.5,0.5))\n",
    "\n",
    "#model prediction\n",
    "prediction_logits = tf.matmul(fc1,W_2)+b_2\n",
    "prediction = tf.nn.softmax(prediction_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)Loss function definiton\n",
    "Both the model use cross entropy as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction_logits, name='cross_entropy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)Create minimizer and train op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")         #variable to keep track of the steps\n",
    "lr = 0.01                                                                 #learning rate\n",
    "\n",
    "train_op = tf.train.MomentumOptimizer(lr,0.90).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to evaluate the performance of the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))   #array of bool\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Create session and train model\n",
    "We will use an interactive session for this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniatialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train for 10000 step with batchsize 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/10000, loss: 2.449557304382324\n",
      "Step 10/10000, loss: 2.182542085647583\n",
      "Step 20/10000, loss: 1.8297770023345947\n",
      "Step 30/10000, loss: 1.3275818824768066\n",
      "Step 40/10000, loss: 1.1607792377471924\n",
      "Step 50/10000, loss: 0.8538472652435303\n",
      "Step 60/10000, loss: 0.679389238357544\n",
      "Step 70/10000, loss: 0.5916299819946289\n",
      "Step 80/10000, loss: 0.6303119659423828\n",
      "Step 90/10000, loss: 0.5767663717269897\n",
      "Step 100/10000, loss: 0.46104270219802856\n",
      "Step 110/10000, loss: 0.48126187920570374\n",
      "Step 120/10000, loss: 0.4936155676841736\n",
      "Step 130/10000, loss: 0.36976659297943115\n",
      "Step 140/10000, loss: 0.4771038293838501\n",
      "Step 150/10000, loss: 0.48835164308547974\n",
      "Step 160/10000, loss: 0.47551801800727844\n",
      "Step 170/10000, loss: 0.5241968035697937\n",
      "Step 180/10000, loss: 0.3776184916496277\n",
      "Step 190/10000, loss: 0.42125171422958374\n",
      "Step 200/10000, loss: 0.4071858823299408\n",
      "Step 210/10000, loss: 0.2790302038192749\n",
      "Step 220/10000, loss: 0.4013800024986267\n",
      "Step 230/10000, loss: 0.3372022807598114\n",
      "Step 240/10000, loss: 0.3469395637512207\n",
      "Step 250/10000, loss: 0.37160658836364746\n",
      "Step 260/10000, loss: 0.40165001153945923\n",
      "Step 270/10000, loss: 0.37877553701400757\n",
      "Step 280/10000, loss: 0.28279930353164673\n",
      "Step 290/10000, loss: 0.39261555671691895\n",
      "Step 300/10000, loss: 0.32876724004745483\n",
      "Step 310/10000, loss: 0.30235010385513306\n",
      "Step 320/10000, loss: 0.2994344234466553\n",
      "Step 330/10000, loss: 0.3894670009613037\n",
      "Step 340/10000, loss: 0.22615133225917816\n",
      "Step 350/10000, loss: 0.23245084285736084\n",
      "Step 360/10000, loss: 0.4204651713371277\n",
      "Step 370/10000, loss: 0.3095502257347107\n",
      "Step 380/10000, loss: 0.39808693528175354\n",
      "Step 390/10000, loss: 0.30332881212234497\n",
      "Step 400/10000, loss: 0.22960489988327026\n",
      "Step 410/10000, loss: 0.34678518772125244\n",
      "Step 420/10000, loss: 0.3501355051994324\n",
      "Step 430/10000, loss: 0.17900852859020233\n",
      "Step 440/10000, loss: 0.28975796699523926\n",
      "Step 450/10000, loss: 0.20610180497169495\n",
      "Step 460/10000, loss: 0.3408074975013733\n",
      "Step 470/10000, loss: 0.2950848340988159\n",
      "Step 480/10000, loss: 0.19733238220214844\n",
      "Step 490/10000, loss: 0.271139919757843\n",
      "Step 500/10000, loss: 0.395997017621994\n",
      "Step 510/10000, loss: 0.38347095251083374\n",
      "Step 520/10000, loss: 0.22068209946155548\n",
      "Step 530/10000, loss: 0.40315157175064087\n",
      "Step 540/10000, loss: 0.3082112967967987\n",
      "Step 550/10000, loss: 0.1487194150686264\n",
      "Step 560/10000, loss: 0.2506110370159149\n",
      "Step 570/10000, loss: 0.43309080600738525\n",
      "Step 580/10000, loss: 0.3627163767814636\n",
      "Step 590/10000, loss: 0.17106452584266663\n",
      "Step 600/10000, loss: 0.21846652030944824\n",
      "Step 610/10000, loss: 0.24982284009456635\n",
      "Step 620/10000, loss: 0.2648887634277344\n",
      "Step 630/10000, loss: 0.19098275899887085\n",
      "Step 640/10000, loss: 0.21763673424720764\n",
      "Step 650/10000, loss: 0.22606714069843292\n",
      "Step 660/10000, loss: 0.2790440022945404\n",
      "Step 670/10000, loss: 0.24416592717170715\n",
      "Step 680/10000, loss: 0.1871751993894577\n",
      "Step 690/10000, loss: 0.2933526337146759\n",
      "Step 700/10000, loss: 0.1364935040473938\n",
      "Step 710/10000, loss: 0.2590677738189697\n",
      "Step 720/10000, loss: 0.24786920845508575\n",
      "Step 730/10000, loss: 0.20704951882362366\n",
      "Step 740/10000, loss: 0.3778982162475586\n",
      "Step 750/10000, loss: 0.2979615330696106\n",
      "Step 760/10000, loss: 0.22471866011619568\n",
      "Step 770/10000, loss: 0.2407325953245163\n",
      "Step 780/10000, loss: 0.4184446930885315\n",
      "Step 790/10000, loss: 0.19788844883441925\n",
      "Step 800/10000, loss: 0.1968497931957245\n",
      "Step 810/10000, loss: 0.2452182173728943\n",
      "Step 820/10000, loss: 0.22294247150421143\n",
      "Step 830/10000, loss: 0.2410101592540741\n",
      "Step 840/10000, loss: 0.3716994524002075\n",
      "Step 850/10000, loss: 0.22086045145988464\n",
      "Step 860/10000, loss: 0.2171230912208557\n",
      "Step 870/10000, loss: 0.3171463906764984\n",
      "Step 880/10000, loss: 0.29498741030693054\n",
      "Step 890/10000, loss: 0.24545103311538696\n",
      "Step 900/10000, loss: 0.16682595014572144\n",
      "Step 910/10000, loss: 0.2279508113861084\n",
      "Step 920/10000, loss: 0.2447524517774582\n",
      "Step 930/10000, loss: 0.23287329077720642\n",
      "Step 940/10000, loss: 0.21938496828079224\n",
      "Step 950/10000, loss: 0.34361687302589417\n",
      "Step 960/10000, loss: 0.19407522678375244\n",
      "Step 970/10000, loss: 0.24675363302230835\n",
      "Step 980/10000, loss: 0.21320109069347382\n",
      "Step 990/10000, loss: 0.19274526834487915\n",
      "Step 1000/10000, loss: 0.15815488994121552\n",
      "Step 1010/10000, loss: 0.2615845203399658\n",
      "Step 1020/10000, loss: 0.3228531777858734\n",
      "Step 1030/10000, loss: 0.15134721994400024\n",
      "Step 1040/10000, loss: 0.18936873972415924\n",
      "Step 1050/10000, loss: 0.22171923518180847\n",
      "Step 1060/10000, loss: 0.19640612602233887\n",
      "Step 1070/10000, loss: 0.24833793938159943\n",
      "Step 1080/10000, loss: 0.25666680932044983\n",
      "Step 1090/10000, loss: 0.25615233182907104\n",
      "Step 1100/10000, loss: 0.21239712834358215\n",
      "Step 1110/10000, loss: 0.11125105619430542\n",
      "Step 1120/10000, loss: 0.21799185872077942\n",
      "Step 1130/10000, loss: 0.09881238639354706\n",
      "Step 1140/10000, loss: 0.22576558589935303\n",
      "Step 1150/10000, loss: 0.299237996339798\n",
      "Step 1160/10000, loss: 0.22251957654953003\n",
      "Step 1170/10000, loss: 0.35519471764564514\n",
      "Step 1180/10000, loss: 0.22458511590957642\n",
      "Step 1190/10000, loss: 0.1618921458721161\n",
      "Step 1200/10000, loss: 0.2236614227294922\n",
      "Step 1210/10000, loss: 0.21389715373516083\n",
      "Step 1220/10000, loss: 0.23263224959373474\n",
      "Step 1230/10000, loss: 0.1601865291595459\n",
      "Step 1240/10000, loss: 0.20201413333415985\n",
      "Step 1250/10000, loss: 0.279666006565094\n",
      "Step 1260/10000, loss: 0.15851278603076935\n",
      "Step 1270/10000, loss: 0.18228964507579803\n",
      "Step 1280/10000, loss: 0.353091299533844\n",
      "Step 1290/10000, loss: 0.20202398300170898\n",
      "Step 1300/10000, loss: 0.14959771931171417\n",
      "Step 1310/10000, loss: 0.21226486563682556\n",
      "Step 1320/10000, loss: 0.14615370333194733\n",
      "Step 1330/10000, loss: 0.16785353422164917\n",
      "Step 1340/10000, loss: 0.27044206857681274\n",
      "Step 1350/10000, loss: 0.16710036993026733\n",
      "Step 1360/10000, loss: 0.3038780987262726\n",
      "Step 1370/10000, loss: 0.17150075733661652\n",
      "Step 1380/10000, loss: 0.1425136923789978\n",
      "Step 1390/10000, loss: 0.2633715569972992\n",
      "Step 1400/10000, loss: 0.21005967259407043\n",
      "Step 1410/10000, loss: 0.1825893670320511\n",
      "Step 1420/10000, loss: 0.21101662516593933\n",
      "Step 1430/10000, loss: 0.20273789763450623\n",
      "Step 1440/10000, loss: 0.18734042346477509\n",
      "Step 1450/10000, loss: 0.17127016186714172\n",
      "Step 1460/10000, loss: 0.17081455886363983\n",
      "Step 1470/10000, loss: 0.2115720808506012\n",
      "Step 1480/10000, loss: 0.29262077808380127\n",
      "Step 1490/10000, loss: 0.19078783690929413\n",
      "Step 1500/10000, loss: 0.18806661665439606\n",
      "Step 1510/10000, loss: 0.1669609248638153\n",
      "Step 1520/10000, loss: 0.1574539989233017\n",
      "Step 1530/10000, loss: 0.13687366247177124\n",
      "Step 1540/10000, loss: 0.17317071557044983\n",
      "Step 1550/10000, loss: 0.12813732028007507\n",
      "Step 1560/10000, loss: 0.15982064604759216\n",
      "Step 1570/10000, loss: 0.15550899505615234\n",
      "Step 1580/10000, loss: 0.24848026037216187\n",
      "Step 1590/10000, loss: 0.17271879315376282\n",
      "Step 1600/10000, loss: 0.2309437394142151\n",
      "Step 1610/10000, loss: 0.1598844826221466\n",
      "Step 1620/10000, loss: 0.13038286566734314\n",
      "Step 1630/10000, loss: 0.14806649088859558\n",
      "Step 1640/10000, loss: 0.27375760674476624\n",
      "Step 1650/10000, loss: 0.17821204662322998\n",
      "Step 1660/10000, loss: 0.26800668239593506\n",
      "Step 1670/10000, loss: 0.20133404433727264\n",
      "Step 1680/10000, loss: 0.1888982355594635\n",
      "Step 1690/10000, loss: 0.1930234134197235\n",
      "Step 1700/10000, loss: 0.2354985475540161\n",
      "Step 1710/10000, loss: 0.17813408374786377\n",
      "Step 1720/10000, loss: 0.23356667160987854\n",
      "Step 1730/10000, loss: 0.27710485458374023\n",
      "Step 1740/10000, loss: 0.17448803782463074\n",
      "Step 1750/10000, loss: 0.23317700624465942\n",
      "Step 1760/10000, loss: 0.13018223643302917\n",
      "Step 1770/10000, loss: 0.17080239951610565\n",
      "Step 1780/10000, loss: 0.10518567264080048\n",
      "Step 1790/10000, loss: 0.1467323899269104\n",
      "Step 1800/10000, loss: 0.22523388266563416\n",
      "Step 1810/10000, loss: 0.19693437218666077\n",
      "Step 1820/10000, loss: 0.2115306854248047\n",
      "Step 1830/10000, loss: 0.18299010396003723\n",
      "Step 1840/10000, loss: 0.08674465119838715\n",
      "Step 1850/10000, loss: 0.19328700006008148\n",
      "Step 1860/10000, loss: 0.197861447930336\n",
      "Step 1870/10000, loss: 0.2268686592578888\n",
      "Step 1880/10000, loss: 0.09389527142047882\n",
      "Step 1890/10000, loss: 0.13517352938652039\n",
      "Step 1900/10000, loss: 0.05960019677877426\n",
      "Step 1910/10000, loss: 0.15470382571220398\n",
      "Step 1920/10000, loss: 0.18044859170913696\n",
      "Step 1930/10000, loss: 0.13251131772994995\n",
      "Step 1940/10000, loss: 0.2742019295692444\n",
      "Step 1950/10000, loss: 0.15091200172901154\n",
      "Step 1960/10000, loss: 0.17136672139167786\n",
      "Step 1970/10000, loss: 0.20569227635860443\n",
      "Step 1980/10000, loss: 0.19370082020759583\n",
      "Step 1990/10000, loss: 0.07304640114307404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000/10000, loss: 0.1835251748561859\n",
      "Step 2010/10000, loss: 0.18698836863040924\n",
      "Step 2020/10000, loss: 0.1592096984386444\n",
      "Step 2030/10000, loss: 0.22621825337409973\n",
      "Step 2040/10000, loss: 0.2840496897697449\n",
      "Step 2050/10000, loss: 0.1995733082294464\n",
      "Step 2060/10000, loss: 0.1408977210521698\n",
      "Step 2070/10000, loss: 0.19498969614505768\n",
      "Step 2080/10000, loss: 0.23531799018383026\n",
      "Step 2090/10000, loss: 0.13976649940013885\n",
      "Step 2100/10000, loss: 0.17340807616710663\n",
      "Step 2110/10000, loss: 0.2040606141090393\n",
      "Step 2120/10000, loss: 0.208665132522583\n",
      "Step 2130/10000, loss: 0.10469008982181549\n",
      "Step 2140/10000, loss: 0.11359798908233643\n",
      "Step 2150/10000, loss: 0.16327187418937683\n",
      "Step 2160/10000, loss: 0.11720076948404312\n",
      "Step 2170/10000, loss: 0.1339198648929596\n",
      "Step 2180/10000, loss: 0.23814795911312103\n",
      "Step 2190/10000, loss: 0.19009491801261902\n",
      "Step 2200/10000, loss: 0.09820844978094101\n",
      "Step 2210/10000, loss: 0.26869285106658936\n",
      "Step 2220/10000, loss: 0.13747656345367432\n",
      "Step 2230/10000, loss: 0.2171390950679779\n",
      "Step 2240/10000, loss: 0.14586077630519867\n",
      "Step 2250/10000, loss: 0.3132914900779724\n",
      "Step 2260/10000, loss: 0.10108667612075806\n",
      "Step 2270/10000, loss: 0.18721359968185425\n",
      "Step 2280/10000, loss: 0.15325820446014404\n",
      "Step 2290/10000, loss: 0.0863313302397728\n",
      "Step 2300/10000, loss: 0.1591150313615799\n",
      "Step 2310/10000, loss: 0.12728820741176605\n",
      "Step 2320/10000, loss: 0.13705047965049744\n",
      "Step 2330/10000, loss: 0.19810396432876587\n",
      "Step 2340/10000, loss: 0.16304177045822144\n",
      "Step 2350/10000, loss: 0.052104104310274124\n",
      "Step 2360/10000, loss: 0.20016460120677948\n",
      "Step 2370/10000, loss: 0.3092939257621765\n",
      "Step 2380/10000, loss: 0.13503901660442352\n",
      "Step 2390/10000, loss: 0.1704348623752594\n",
      "Step 2400/10000, loss: 0.2486530840396881\n",
      "Step 2410/10000, loss: 0.13856804370880127\n",
      "Step 2420/10000, loss: 0.22939687967300415\n",
      "Step 2430/10000, loss: 0.08235593140125275\n",
      "Step 2440/10000, loss: 0.1260247528553009\n",
      "Step 2450/10000, loss: 0.16709038615226746\n",
      "Step 2460/10000, loss: 0.1490551382303238\n",
      "Step 2470/10000, loss: 0.29002612829208374\n",
      "Step 2480/10000, loss: 0.23881250619888306\n",
      "Step 2490/10000, loss: 0.18673837184906006\n",
      "Step 2500/10000, loss: 0.1427564024925232\n",
      "Step 2510/10000, loss: 0.22624079883098602\n",
      "Step 2520/10000, loss: 0.17208650708198547\n",
      "Step 2530/10000, loss: 0.19507819414138794\n",
      "Step 2540/10000, loss: 0.1972857266664505\n",
      "Step 2550/10000, loss: 0.14829528331756592\n",
      "Step 2560/10000, loss: 0.27267158031463623\n",
      "Step 2570/10000, loss: 0.09524591267108917\n",
      "Step 2580/10000, loss: 0.1945170909166336\n",
      "Step 2590/10000, loss: 0.2149297595024109\n",
      "Step 2600/10000, loss: 0.09610855579376221\n",
      "Step 2610/10000, loss: 0.06538905203342438\n",
      "Step 2620/10000, loss: 0.12937289476394653\n",
      "Step 2630/10000, loss: 0.24141453206539154\n",
      "Step 2640/10000, loss: 0.06964154541492462\n",
      "Step 2650/10000, loss: 0.10042775422334671\n",
      "Step 2660/10000, loss: 0.06104825809597969\n",
      "Step 2670/10000, loss: 0.14135386049747467\n",
      "Step 2680/10000, loss: 0.11809338629245758\n",
      "Step 2690/10000, loss: 0.1331726610660553\n",
      "Step 2700/10000, loss: 0.19007042050361633\n",
      "Step 2710/10000, loss: 0.12261347472667694\n",
      "Step 2720/10000, loss: 0.11407210677862167\n",
      "Step 2730/10000, loss: 0.08640017360448837\n",
      "Step 2740/10000, loss: 0.15836921334266663\n",
      "Step 2750/10000, loss: 0.15109199285507202\n",
      "Step 2760/10000, loss: 0.09363913536071777\n",
      "Step 2770/10000, loss: 0.07044944912195206\n",
      "Step 2780/10000, loss: 0.14785142242908478\n",
      "Step 2790/10000, loss: 0.1401016265153885\n",
      "Step 2800/10000, loss: 0.10616215318441391\n",
      "Step 2810/10000, loss: 0.19478680193424225\n",
      "Step 2820/10000, loss: 0.12635499238967896\n",
      "Step 2830/10000, loss: 0.10039670020341873\n",
      "Step 2840/10000, loss: 0.18617716431617737\n",
      "Step 2850/10000, loss: 0.15454821288585663\n",
      "Step 2860/10000, loss: 0.14278337359428406\n",
      "Step 2870/10000, loss: 0.13442738354206085\n",
      "Step 2880/10000, loss: 0.17487312853336334\n",
      "Step 2890/10000, loss: 0.18555419147014618\n",
      "Step 2900/10000, loss: 0.1594085395336151\n",
      "Step 2910/10000, loss: 0.09753352403640747\n",
      "Step 2920/10000, loss: 0.17295637726783752\n",
      "Step 2930/10000, loss: 0.2484949827194214\n",
      "Step 2940/10000, loss: 0.1254493147134781\n",
      "Step 2950/10000, loss: 0.12459150701761246\n",
      "Step 2960/10000, loss: 0.16369852423667908\n",
      "Step 2970/10000, loss: 0.08837620913982391\n",
      "Step 2980/10000, loss: 0.1449296772480011\n",
      "Step 2990/10000, loss: 0.06807908415794373\n",
      "Step 3000/10000, loss: 0.09572097659111023\n",
      "Step 3010/10000, loss: 0.1353980153799057\n",
      "Step 3020/10000, loss: 0.0689484104514122\n",
      "Step 3030/10000, loss: 0.16932272911071777\n",
      "Step 3040/10000, loss: 0.1447993367910385\n",
      "Step 3050/10000, loss: 0.10630597174167633\n",
      "Step 3060/10000, loss: 0.04322332143783569\n",
      "Step 3070/10000, loss: 0.17907139658927917\n",
      "Step 3080/10000, loss: 0.16085705161094666\n",
      "Step 3090/10000, loss: 0.20071718096733093\n",
      "Step 3100/10000, loss: 0.24384042620658875\n",
      "Step 3110/10000, loss: 0.1155465692281723\n",
      "Step 3120/10000, loss: 0.17338693141937256\n",
      "Step 3130/10000, loss: 0.19160829484462738\n",
      "Step 3140/10000, loss: 0.08651931583881378\n",
      "Step 3150/10000, loss: 0.10080793499946594\n",
      "Step 3160/10000, loss: 0.14323000609874725\n",
      "Step 3170/10000, loss: 0.11203460395336151\n",
      "Step 3180/10000, loss: 0.12667763233184814\n",
      "Step 3190/10000, loss: 0.14159850776195526\n",
      "Step 3200/10000, loss: 0.18943750858306885\n",
      "Step 3210/10000, loss: 0.14341293275356293\n",
      "Step 3220/10000, loss: 0.1006876677274704\n",
      "Step 3230/10000, loss: 0.07793888449668884\n",
      "Step 3240/10000, loss: 0.18284186720848083\n",
      "Step 3250/10000, loss: 0.09306111931800842\n",
      "Step 3260/10000, loss: 0.15620829164981842\n",
      "Step 3270/10000, loss: 0.19056233763694763\n",
      "Step 3280/10000, loss: 0.0861329734325409\n",
      "Step 3290/10000, loss: 0.0890200287103653\n",
      "Step 3300/10000, loss: 0.08828767389059067\n",
      "Step 3310/10000, loss: 0.0906088575720787\n",
      "Step 3320/10000, loss: 0.10202085226774216\n",
      "Step 3330/10000, loss: 0.14643487334251404\n",
      "Step 3340/10000, loss: 0.11274967342615128\n",
      "Step 3350/10000, loss: 0.08214709907770157\n",
      "Step 3360/10000, loss: 0.1461363136768341\n",
      "Step 3370/10000, loss: 0.08956491947174072\n",
      "Step 3380/10000, loss: 0.16924026608467102\n",
      "Step 3390/10000, loss: 0.12862634658813477\n",
      "Step 3400/10000, loss: 0.11022242158651352\n",
      "Step 3410/10000, loss: 0.13782428205013275\n",
      "Step 3420/10000, loss: 0.09821007400751114\n",
      "Step 3430/10000, loss: 0.21641290187835693\n",
      "Step 3440/10000, loss: 0.06027759611606598\n",
      "Step 3450/10000, loss: 0.10698144882917404\n",
      "Step 3460/10000, loss: 0.26171886920928955\n",
      "Step 3470/10000, loss: 0.10428772866725922\n",
      "Step 3480/10000, loss: 0.07757769525051117\n",
      "Step 3490/10000, loss: 0.08135199546813965\n",
      "Step 3500/10000, loss: 0.18779057264328003\n",
      "Step 3510/10000, loss: 0.1265970766544342\n",
      "Step 3520/10000, loss: 0.10839773714542389\n",
      "Step 3530/10000, loss: 0.13427981734275818\n",
      "Step 3540/10000, loss: 0.07393252104520798\n",
      "Step 3550/10000, loss: 0.0936359167098999\n",
      "Step 3560/10000, loss: 0.07185010612010956\n",
      "Step 3570/10000, loss: 0.11840138584375381\n",
      "Step 3580/10000, loss: 0.11867474019527435\n",
      "Step 3590/10000, loss: 0.14353813230991364\n",
      "Step 3600/10000, loss: 0.09144754707813263\n",
      "Step 3610/10000, loss: 0.062272168695926666\n",
      "Step 3620/10000, loss: 0.16766779124736786\n",
      "Step 3630/10000, loss: 0.15501359105110168\n",
      "Step 3640/10000, loss: 0.06533828377723694\n",
      "Step 3650/10000, loss: 0.14692920446395874\n",
      "Step 3660/10000, loss: 0.09006292372941971\n",
      "Step 3670/10000, loss: 0.07360489666461945\n",
      "Step 3680/10000, loss: 0.12112508714199066\n",
      "Step 3690/10000, loss: 0.13815414905548096\n",
      "Step 3700/10000, loss: 0.21090003848075867\n",
      "Step 3710/10000, loss: 0.12265557050704956\n",
      "Step 3720/10000, loss: 0.10139548033475876\n",
      "Step 3730/10000, loss: 0.1523609161376953\n",
      "Step 3740/10000, loss: 0.06369003653526306\n",
      "Step 3750/10000, loss: 0.08622179925441742\n",
      "Step 3760/10000, loss: 0.3015385866165161\n",
      "Step 3770/10000, loss: 0.1556570827960968\n",
      "Step 3780/10000, loss: 0.03684138506650925\n",
      "Step 3790/10000, loss: 0.2608761489391327\n",
      "Step 3800/10000, loss: 0.08309893310070038\n",
      "Step 3810/10000, loss: 0.10220463573932648\n",
      "Step 3820/10000, loss: 0.1251482516527176\n",
      "Step 3830/10000, loss: 0.13397350907325745\n",
      "Step 3840/10000, loss: 0.08516845852136612\n",
      "Step 3850/10000, loss: 0.1760827898979187\n",
      "Step 3860/10000, loss: 0.05037524551153183\n",
      "Step 3870/10000, loss: 0.18711644411087036\n",
      "Step 3880/10000, loss: 0.0988457128405571\n",
      "Step 3890/10000, loss: 0.18223325908184052\n",
      "Step 3900/10000, loss: 0.0803258940577507\n",
      "Step 3910/10000, loss: 0.14578500390052795\n",
      "Step 3920/10000, loss: 0.18383194506168365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3930/10000, loss: 0.1458199918270111\n",
      "Step 3940/10000, loss: 0.12903231382369995\n",
      "Step 3950/10000, loss: 0.09598380327224731\n",
      "Step 3960/10000, loss: 0.2623329162597656\n",
      "Step 3970/10000, loss: 0.09765283018350601\n",
      "Step 3980/10000, loss: 0.13584867119789124\n",
      "Step 3990/10000, loss: 0.10234393179416656\n",
      "Step 4000/10000, loss: 0.07193988561630249\n",
      "Step 4010/10000, loss: 0.17474079132080078\n",
      "Step 4020/10000, loss: 0.10479037463665009\n",
      "Step 4030/10000, loss: 0.16114583611488342\n",
      "Step 4040/10000, loss: 0.048332877457141876\n",
      "Step 4050/10000, loss: 0.07911097258329391\n",
      "Step 4060/10000, loss: 0.0868566706776619\n",
      "Step 4070/10000, loss: 0.1114044040441513\n",
      "Step 4080/10000, loss: 0.14751194417476654\n",
      "Step 4090/10000, loss: 0.0878944844007492\n",
      "Step 4100/10000, loss: 0.1511003077030182\n",
      "Step 4110/10000, loss: 0.0594756156206131\n",
      "Step 4120/10000, loss: 0.08977627754211426\n",
      "Step 4130/10000, loss: 0.10999732464551926\n",
      "Step 4140/10000, loss: 0.09759892523288727\n",
      "Step 4150/10000, loss: 0.06078631430864334\n",
      "Step 4160/10000, loss: 0.11728736758232117\n",
      "Step 4170/10000, loss: 0.1734038144350052\n",
      "Step 4180/10000, loss: 0.17262691259384155\n",
      "Step 4190/10000, loss: 0.04837546870112419\n",
      "Step 4200/10000, loss: 0.07467973977327347\n",
      "Step 4210/10000, loss: 0.13307657837867737\n",
      "Step 4220/10000, loss: 0.08601155877113342\n",
      "Step 4230/10000, loss: 0.05229630693793297\n",
      "Step 4240/10000, loss: 0.15372012555599213\n",
      "Step 4250/10000, loss: 0.043528277426958084\n",
      "Step 4260/10000, loss: 0.11211962252855301\n",
      "Step 4270/10000, loss: 0.09266996383666992\n",
      "Step 4280/10000, loss: 0.11568425595760345\n",
      "Step 4290/10000, loss: 0.08562416583299637\n",
      "Step 4300/10000, loss: 0.10334022343158722\n",
      "Step 4310/10000, loss: 0.09486501663923264\n",
      "Step 4320/10000, loss: 0.10188627243041992\n",
      "Step 4330/10000, loss: 0.14428837597370148\n",
      "Step 4340/10000, loss: 0.0899219736456871\n",
      "Step 4350/10000, loss: 0.0963168740272522\n",
      "Step 4360/10000, loss: 0.14631083607673645\n",
      "Step 4370/10000, loss: 0.10683874785900116\n",
      "Step 4380/10000, loss: 0.18957792222499847\n",
      "Step 4390/10000, loss: 0.19796091318130493\n",
      "Step 4400/10000, loss: 0.1392035186290741\n",
      "Step 4410/10000, loss: 0.07645691186189651\n",
      "Step 4420/10000, loss: 0.04098305106163025\n",
      "Step 4430/10000, loss: 0.15986694395542145\n",
      "Step 4440/10000, loss: 0.14298588037490845\n",
      "Step 4450/10000, loss: 0.09786689281463623\n",
      "Step 4460/10000, loss: 0.11207490414381027\n",
      "Step 4470/10000, loss: 0.167566180229187\n",
      "Step 4480/10000, loss: 0.1903921663761139\n",
      "Step 4490/10000, loss: 0.0766788050532341\n",
      "Step 4500/10000, loss: 0.09306886792182922\n",
      "Step 4510/10000, loss: 0.05507185310125351\n",
      "Step 4520/10000, loss: 0.0595337450504303\n",
      "Step 4530/10000, loss: 0.12382541596889496\n",
      "Step 4540/10000, loss: 0.11893188208341599\n",
      "Step 4550/10000, loss: 0.098419189453125\n",
      "Step 4560/10000, loss: 0.07724972069263458\n",
      "Step 4570/10000, loss: 0.13855485618114471\n",
      "Step 4580/10000, loss: 0.10831937938928604\n",
      "Step 4590/10000, loss: 0.14890354871749878\n",
      "Step 4600/10000, loss: 0.046930618584156036\n",
      "Step 4610/10000, loss: 0.11794738471508026\n",
      "Step 4620/10000, loss: 0.11489582061767578\n",
      "Step 4630/10000, loss: 0.10038428008556366\n",
      "Step 4640/10000, loss: 0.05502547323703766\n",
      "Step 4650/10000, loss: 0.15256023406982422\n",
      "Step 4660/10000, loss: 0.2030581831932068\n",
      "Step 4670/10000, loss: 0.11660683155059814\n",
      "Step 4680/10000, loss: 0.14503899216651917\n",
      "Step 4690/10000, loss: 0.03771830350160599\n",
      "Step 4700/10000, loss: 0.09190471470355988\n",
      "Step 4710/10000, loss: 0.05664926394820213\n",
      "Step 4720/10000, loss: 0.07566896080970764\n",
      "Step 4730/10000, loss: 0.1092284694314003\n",
      "Step 4740/10000, loss: 0.08003425598144531\n",
      "Step 4750/10000, loss: 0.10817939788103104\n",
      "Step 4760/10000, loss: 0.1508171558380127\n",
      "Step 4770/10000, loss: 0.040132276713848114\n",
      "Step 4780/10000, loss: 0.056220151484012604\n",
      "Step 4790/10000, loss: 0.04565725103020668\n",
      "Step 4800/10000, loss: 0.07942584156990051\n",
      "Step 4810/10000, loss: 0.09474443644285202\n",
      "Step 4820/10000, loss: 0.11361828446388245\n",
      "Step 4830/10000, loss: 0.17638742923736572\n",
      "Step 4840/10000, loss: 0.10793551802635193\n",
      "Step 4850/10000, loss: 0.0759659856557846\n",
      "Step 4860/10000, loss: 0.11609266698360443\n",
      "Step 4870/10000, loss: 0.12376652657985687\n",
      "Step 4880/10000, loss: 0.13354483246803284\n",
      "Step 4890/10000, loss: 0.04844962805509567\n",
      "Step 4900/10000, loss: 0.08318205177783966\n",
      "Step 4910/10000, loss: 0.0985352098941803\n",
      "Step 4920/10000, loss: 0.08257029950618744\n",
      "Step 4930/10000, loss: 0.08386990427970886\n",
      "Step 4940/10000, loss: 0.15692488849163055\n",
      "Step 4950/10000, loss: 0.10642396658658981\n",
      "Step 4960/10000, loss: 0.08589702099561691\n",
      "Step 4970/10000, loss: 0.05727231875061989\n",
      "Step 4980/10000, loss: 0.15190257132053375\n",
      "Step 4990/10000, loss: 0.13771426677703857\n",
      "Step 5000/10000, loss: 0.1092609316110611\n",
      "Step 5010/10000, loss: 0.08734188973903656\n",
      "Step 5020/10000, loss: 0.07740374654531479\n",
      "Step 5030/10000, loss: 0.08537853509187698\n",
      "Step 5040/10000, loss: 0.0753650963306427\n",
      "Step 5050/10000, loss: 0.1604771912097931\n",
      "Step 5060/10000, loss: 0.11448640376329422\n",
      "Step 5070/10000, loss: 0.16240772604942322\n",
      "Step 5080/10000, loss: 0.18007990717887878\n",
      "Step 5090/10000, loss: 0.20789548754692078\n",
      "Step 5100/10000, loss: 0.12324778735637665\n",
      "Step 5110/10000, loss: 0.05477255582809448\n",
      "Step 5120/10000, loss: 0.04559741169214249\n",
      "Step 5130/10000, loss: 0.1270536184310913\n",
      "Step 5140/10000, loss: 0.11936645954847336\n",
      "Step 5150/10000, loss: 0.11896663904190063\n",
      "Step 5160/10000, loss: 0.09069155901670456\n",
      "Step 5170/10000, loss: 0.07878340035676956\n",
      "Step 5180/10000, loss: 0.0842527449131012\n",
      "Step 5190/10000, loss: 0.05902928113937378\n",
      "Step 5200/10000, loss: 0.123406320810318\n",
      "Step 5210/10000, loss: 0.07165657728910446\n",
      "Step 5220/10000, loss: 0.07053226232528687\n",
      "Step 5230/10000, loss: 0.07989367842674255\n",
      "Step 5240/10000, loss: 0.11074119806289673\n",
      "Step 5250/10000, loss: 0.08540931344032288\n",
      "Step 5260/10000, loss: 0.08337027579545975\n",
      "Step 5270/10000, loss: 0.1235201433300972\n",
      "Step 5280/10000, loss: 0.03392748162150383\n",
      "Step 5290/10000, loss: 0.055293336510658264\n",
      "Step 5300/10000, loss: 0.10870207846164703\n",
      "Step 5310/10000, loss: 0.20320449769496918\n",
      "Step 5320/10000, loss: 0.05744967609643936\n",
      "Step 5330/10000, loss: 0.08824625611305237\n",
      "Step 5340/10000, loss: 0.10629936307668686\n",
      "Step 5350/10000, loss: 0.09029033035039902\n",
      "Step 5360/10000, loss: 0.07507029920816422\n",
      "Step 5370/10000, loss: 0.0501650795340538\n",
      "Step 5380/10000, loss: 0.07556118071079254\n",
      "Step 5390/10000, loss: 0.0853600949048996\n",
      "Step 5400/10000, loss: 0.12435802817344666\n",
      "Step 5410/10000, loss: 0.09137319028377533\n",
      "Step 5420/10000, loss: 0.10294374823570251\n",
      "Step 5430/10000, loss: 0.1134423017501831\n",
      "Step 5440/10000, loss: 0.08883462101221085\n",
      "Step 5450/10000, loss: 0.07880634069442749\n",
      "Step 5460/10000, loss: 0.04922272264957428\n",
      "Step 5470/10000, loss: 0.042718470096588135\n",
      "Step 5480/10000, loss: 0.1257733553647995\n",
      "Step 5490/10000, loss: 0.14923891425132751\n",
      "Step 5500/10000, loss: 0.13822847604751587\n",
      "Step 5510/10000, loss: 0.06169089674949646\n",
      "Step 5520/10000, loss: 0.08030015230178833\n",
      "Step 5530/10000, loss: 0.038265302777290344\n",
      "Step 5540/10000, loss: 0.10829094052314758\n",
      "Step 5550/10000, loss: 0.030540136620402336\n",
      "Step 5560/10000, loss: 0.17943747341632843\n",
      "Step 5570/10000, loss: 0.06338454782962799\n",
      "Step 5580/10000, loss: 0.07660556584596634\n",
      "Step 5590/10000, loss: 0.05469520390033722\n",
      "Step 5600/10000, loss: 0.14612501859664917\n",
      "Step 5610/10000, loss: 0.08770976215600967\n",
      "Step 5620/10000, loss: 0.09186865389347076\n",
      "Step 5630/10000, loss: 0.04514294117689133\n",
      "Step 5640/10000, loss: 0.11938342452049255\n",
      "Step 5650/10000, loss: 0.049880728125572205\n",
      "Step 5660/10000, loss: 0.03217511624097824\n",
      "Step 5670/10000, loss: 0.07830162346363068\n",
      "Step 5680/10000, loss: 0.0493956133723259\n",
      "Step 5690/10000, loss: 0.03901326656341553\n",
      "Step 5700/10000, loss: 0.04343253746628761\n",
      "Step 5710/10000, loss: 0.07554199546575546\n",
      "Step 5720/10000, loss: 0.184663325548172\n",
      "Step 5730/10000, loss: 0.08589393645524979\n",
      "Step 5740/10000, loss: 0.05462225526571274\n",
      "Step 5750/10000, loss: 0.1174403578042984\n",
      "Step 5760/10000, loss: 0.10898159444332123\n",
      "Step 5770/10000, loss: 0.15197139978408813\n",
      "Step 5780/10000, loss: 0.08904214948415756\n",
      "Step 5790/10000, loss: 0.048297278583049774\n",
      "Step 5800/10000, loss: 0.13512662053108215\n",
      "Step 5810/10000, loss: 0.05737462639808655\n",
      "Step 5820/10000, loss: 0.07196754217147827\n",
      "Step 5830/10000, loss: 0.120827317237854\n",
      "Step 5840/10000, loss: 0.12840192019939423\n",
      "Step 5850/10000, loss: 0.03728119656443596\n",
      "Step 5860/10000, loss: 0.10900234431028366\n",
      "Step 5870/10000, loss: 0.10258030146360397\n",
      "Step 5880/10000, loss: 0.06299258768558502\n",
      "Step 5890/10000, loss: 0.08279093354940414\n",
      "Step 5900/10000, loss: 0.11822936683893204\n",
      "Step 5910/10000, loss: 0.0620514415204525\n",
      "Step 5920/10000, loss: 0.21922528743743896\n",
      "Step 5930/10000, loss: 0.14626875519752502\n",
      "Step 5940/10000, loss: 0.05857407674193382\n",
      "Step 5950/10000, loss: 0.08053593337535858\n",
      "Step 5960/10000, loss: 0.09112445265054703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5970/10000, loss: 0.06991983950138092\n",
      "Step 5980/10000, loss: 0.06668239086866379\n",
      "Step 5990/10000, loss: 0.07139542698860168\n",
      "Step 6000/10000, loss: 0.05624847114086151\n",
      "Step 6010/10000, loss: 0.19111625850200653\n",
      "Step 6020/10000, loss: 0.10888759791851044\n",
      "Step 6030/10000, loss: 0.10986340790987015\n",
      "Step 6040/10000, loss: 0.07654661685228348\n",
      "Step 6050/10000, loss: 0.12401153147220612\n",
      "Step 6060/10000, loss: 0.09342556446790695\n",
      "Step 6070/10000, loss: 0.06018305942416191\n",
      "Step 6080/10000, loss: 0.09461653977632523\n",
      "Step 6090/10000, loss: 0.03776615113019943\n",
      "Step 6100/10000, loss: 0.12096048891544342\n",
      "Step 6110/10000, loss: 0.07792699337005615\n",
      "Step 6120/10000, loss: 0.09894614666700363\n",
      "Step 6130/10000, loss: 0.05657016485929489\n",
      "Step 6140/10000, loss: 0.09691457450389862\n",
      "Step 6150/10000, loss: 0.07648950815200806\n",
      "Step 6160/10000, loss: 0.06965337693691254\n",
      "Step 6170/10000, loss: 0.1388685554265976\n",
      "Step 6180/10000, loss: 0.03937918320298195\n",
      "Step 6190/10000, loss: 0.07816032320261002\n",
      "Step 6200/10000, loss: 0.23441356420516968\n",
      "Step 6210/10000, loss: 0.0730191320180893\n",
      "Step 6220/10000, loss: 0.07350438833236694\n",
      "Step 6230/10000, loss: 0.05296386033296585\n",
      "Step 6240/10000, loss: 0.08929510414600372\n",
      "Step 6250/10000, loss: 0.05888350307941437\n",
      "Step 6260/10000, loss: 0.07377571612596512\n",
      "Step 6270/10000, loss: 0.07198471575975418\n",
      "Step 6280/10000, loss: 0.09023741632699966\n",
      "Step 6290/10000, loss: 0.20530211925506592\n",
      "Step 6300/10000, loss: 0.13594521582126617\n",
      "Step 6310/10000, loss: 0.11400850117206573\n",
      "Step 6320/10000, loss: 0.05045008659362793\n",
      "Step 6330/10000, loss: 0.0562356635928154\n",
      "Step 6340/10000, loss: 0.07488581538200378\n",
      "Step 6350/10000, loss: 0.09017323702573776\n",
      "Step 6360/10000, loss: 0.10187375545501709\n",
      "Step 6370/10000, loss: 0.03807621821761131\n",
      "Step 6380/10000, loss: 0.13908438384532928\n",
      "Step 6390/10000, loss: 0.13413092494010925\n",
      "Step 6400/10000, loss: 0.1044393852353096\n",
      "Step 6410/10000, loss: 0.09094740450382233\n",
      "Step 6420/10000, loss: 0.04853610694408417\n",
      "Step 6430/10000, loss: 0.10363571345806122\n",
      "Step 6440/10000, loss: 0.05986496061086655\n",
      "Step 6450/10000, loss: 0.1445198506116867\n",
      "Step 6460/10000, loss: 0.09662917256355286\n",
      "Step 6470/10000, loss: 0.1064695343375206\n",
      "Step 6480/10000, loss: 0.09085060656070709\n",
      "Step 6490/10000, loss: 0.09998508542776108\n",
      "Step 6500/10000, loss: 0.04216921329498291\n",
      "Step 6510/10000, loss: 0.0956917554140091\n",
      "Step 6520/10000, loss: 0.08942902833223343\n",
      "Step 6530/10000, loss: 0.10134822875261307\n",
      "Step 6540/10000, loss: 0.08925186097621918\n",
      "Step 6550/10000, loss: 0.1428217589855194\n",
      "Step 6560/10000, loss: 0.07911991328001022\n",
      "Step 6570/10000, loss: 0.06942077726125717\n",
      "Step 6580/10000, loss: 0.05125203728675842\n",
      "Step 6590/10000, loss: 0.0628969818353653\n",
      "Step 6600/10000, loss: 0.09377652406692505\n",
      "Step 6610/10000, loss: 0.06552572548389435\n",
      "Step 6620/10000, loss: 0.12240002304315567\n",
      "Step 6630/10000, loss: 0.06681676208972931\n",
      "Step 6640/10000, loss: 0.07746006548404694\n",
      "Step 6650/10000, loss: 0.12045558542013168\n",
      "Step 6660/10000, loss: 0.04581030458211899\n",
      "Step 6670/10000, loss: 0.12182757258415222\n",
      "Step 6680/10000, loss: 0.028624719008803368\n",
      "Step 6690/10000, loss: 0.08558204770088196\n",
      "Step 6700/10000, loss: 0.10008196532726288\n",
      "Step 6710/10000, loss: 0.0932324230670929\n",
      "Step 6720/10000, loss: 0.03294716775417328\n",
      "Step 6730/10000, loss: 0.05479623004794121\n",
      "Step 6740/10000, loss: 0.08705653995275497\n",
      "Step 6750/10000, loss: 0.03400979936122894\n",
      "Step 6760/10000, loss: 0.05676348879933357\n",
      "Step 6770/10000, loss: 0.07648687809705734\n",
      "Step 6780/10000, loss: 0.06482423841953278\n",
      "Step 6790/10000, loss: 0.05980127304792404\n",
      "Step 6800/10000, loss: 0.04630674421787262\n",
      "Step 6810/10000, loss: 0.12181153148412704\n",
      "Step 6820/10000, loss: 0.1534932553768158\n",
      "Step 6830/10000, loss: 0.06948162615299225\n",
      "Step 6840/10000, loss: 0.11976352334022522\n",
      "Step 6850/10000, loss: 0.06842434406280518\n",
      "Step 6860/10000, loss: 0.0881938487291336\n",
      "Step 6870/10000, loss: 0.08673200756311417\n",
      "Step 6880/10000, loss: 0.1311459243297577\n",
      "Step 6890/10000, loss: 0.09546693414449692\n",
      "Step 6900/10000, loss: 0.10528243333101273\n",
      "Step 6910/10000, loss: 0.07188989222049713\n",
      "Step 6920/10000, loss: 0.04228317365050316\n",
      "Step 6930/10000, loss: 0.07896935194730759\n",
      "Step 6940/10000, loss: 0.05289152264595032\n",
      "Step 6950/10000, loss: 0.0912105143070221\n",
      "Step 6960/10000, loss: 0.082779161632061\n",
      "Step 6970/10000, loss: 0.05507602542638779\n",
      "Step 6980/10000, loss: 0.08372540026903152\n",
      "Step 6990/10000, loss: 0.12159867584705353\n",
      "Step 7000/10000, loss: 0.12943391501903534\n",
      "Step 7010/10000, loss: 0.09440964460372925\n",
      "Step 7020/10000, loss: 0.09222365915775299\n",
      "Step 7030/10000, loss: 0.037812989205121994\n",
      "Step 7040/10000, loss: 0.08777416497468948\n",
      "Step 7050/10000, loss: 0.13764289021492004\n",
      "Step 7060/10000, loss: 0.09302058815956116\n",
      "Step 7070/10000, loss: 0.07507933676242828\n",
      "Step 7080/10000, loss: 0.09766314923763275\n",
      "Step 7090/10000, loss: 0.05639375001192093\n",
      "Step 7100/10000, loss: 0.07073318958282471\n",
      "Step 7110/10000, loss: 0.08668065816164017\n",
      "Step 7120/10000, loss: 0.09182437509298325\n",
      "Step 7130/10000, loss: 0.05801745504140854\n",
      "Step 7140/10000, loss: 0.06351110339164734\n",
      "Step 7150/10000, loss: 0.07312899082899094\n",
      "Step 7160/10000, loss: 0.0415986031293869\n",
      "Step 7170/10000, loss: 0.0453791618347168\n",
      "Step 7180/10000, loss: 0.12725387513637543\n",
      "Step 7190/10000, loss: 0.09496176987886429\n",
      "Step 7200/10000, loss: 0.06220705062150955\n",
      "Step 7210/10000, loss: 0.06452121585607529\n",
      "Step 7220/10000, loss: 0.09133506566286087\n",
      "Step 7230/10000, loss: 0.09058654308319092\n",
      "Step 7240/10000, loss: 0.1259976327419281\n",
      "Step 7250/10000, loss: 0.04035778343677521\n",
      "Step 7260/10000, loss: 0.1308947056531906\n",
      "Step 7270/10000, loss: 0.0679718628525734\n",
      "Step 7280/10000, loss: 0.095778688788414\n",
      "Step 7290/10000, loss: 0.06685318797826767\n",
      "Step 7300/10000, loss: 0.036715444177389145\n",
      "Step 7310/10000, loss: 0.035072244703769684\n",
      "Step 7320/10000, loss: 0.0876547321677208\n",
      "Step 7330/10000, loss: 0.04161737859249115\n",
      "Step 7340/10000, loss: 0.06597422063350677\n",
      "Step 7350/10000, loss: 0.06463736295700073\n",
      "Step 7360/10000, loss: 0.07541386783123016\n",
      "Step 7370/10000, loss: 0.04675115644931793\n",
      "Step 7380/10000, loss: 0.09506653249263763\n",
      "Step 7390/10000, loss: 0.09603265672922134\n",
      "Step 7400/10000, loss: 0.0918092131614685\n",
      "Step 7410/10000, loss: 0.09764210879802704\n",
      "Step 7420/10000, loss: 0.07582815736532211\n",
      "Step 7430/10000, loss: 0.040428534150123596\n",
      "Step 7440/10000, loss: 0.06613856554031372\n",
      "Step 7450/10000, loss: 0.06042575463652611\n",
      "Step 7460/10000, loss: 0.08267100155353546\n",
      "Step 7470/10000, loss: 0.04055582359433174\n",
      "Step 7480/10000, loss: 0.051830604672431946\n",
      "Step 7490/10000, loss: 0.08055004477500916\n",
      "Step 7500/10000, loss: 0.03832006826996803\n",
      "Step 7510/10000, loss: 0.08416388928890228\n",
      "Step 7520/10000, loss: 0.07747530937194824\n",
      "Step 7530/10000, loss: 0.07253922522068024\n",
      "Step 7540/10000, loss: 0.13011318445205688\n",
      "Step 7550/10000, loss: 0.05062904953956604\n",
      "Step 7560/10000, loss: 0.09510188549757004\n",
      "Step 7570/10000, loss: 0.04577437415719032\n",
      "Step 7580/10000, loss: 0.08564430475234985\n",
      "Step 7590/10000, loss: 0.06378795206546783\n",
      "Step 7600/10000, loss: 0.07890436798334122\n",
      "Step 7610/10000, loss: 0.047965578734874725\n",
      "Step 7620/10000, loss: 0.0636959969997406\n",
      "Step 7630/10000, loss: 0.16894365847110748\n",
      "Step 7640/10000, loss: 0.13019001483917236\n",
      "Step 7650/10000, loss: 0.14301256835460663\n",
      "Step 7660/10000, loss: 0.12907077372074127\n",
      "Step 7670/10000, loss: 0.09953460097312927\n",
      "Step 7680/10000, loss: 0.0647287666797638\n",
      "Step 7690/10000, loss: 0.038874492049217224\n",
      "Step 7700/10000, loss: 0.07323292642831802\n",
      "Step 7710/10000, loss: 0.06761981546878815\n",
      "Step 7720/10000, loss: 0.09949466586112976\n",
      "Step 7730/10000, loss: 0.11081887036561966\n",
      "Step 7740/10000, loss: 0.06517279148101807\n",
      "Step 7750/10000, loss: 0.039737239480018616\n",
      "Step 7760/10000, loss: 0.08321543782949448\n",
      "Step 7770/10000, loss: 0.07818081229925156\n",
      "Step 7780/10000, loss: 0.09179512411355972\n",
      "Step 7790/10000, loss: 0.05327857658267021\n",
      "Step 7800/10000, loss: 0.02121400460600853\n",
      "Step 7810/10000, loss: 0.0814172774553299\n",
      "Step 7820/10000, loss: 0.08005346357822418\n",
      "Step 7830/10000, loss: 0.052559271454811096\n",
      "Step 7840/10000, loss: 0.05407800152897835\n",
      "Step 7850/10000, loss: 0.03804977610707283\n",
      "Step 7860/10000, loss: 0.07940667867660522\n",
      "Step 7870/10000, loss: 0.03125932812690735\n",
      "Step 7880/10000, loss: 0.059077732264995575\n",
      "Step 7890/10000, loss: 0.05433293431997299\n",
      "Step 7900/10000, loss: 0.07727712392807007\n",
      "Step 7910/10000, loss: 0.06279440969228745\n",
      "Step 7920/10000, loss: 0.03593868762254715\n",
      "Step 7930/10000, loss: 0.036061160266399384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7940/10000, loss: 0.05847528204321861\n",
      "Step 7950/10000, loss: 0.20926141738891602\n",
      "Step 7960/10000, loss: 0.042049385607242584\n",
      "Step 7970/10000, loss: 0.04033728316426277\n",
      "Step 7980/10000, loss: 0.08089189231395721\n",
      "Step 7990/10000, loss: 0.1073131114244461\n",
      "Step 8000/10000, loss: 0.0934055969119072\n",
      "Step 8010/10000, loss: 0.05872044712305069\n",
      "Step 8020/10000, loss: 0.04542503505945206\n",
      "Step 8030/10000, loss: 0.11885100603103638\n",
      "Step 8040/10000, loss: 0.04715881869196892\n",
      "Step 8050/10000, loss: 0.03077780455350876\n",
      "Step 8060/10000, loss: 0.05616871267557144\n",
      "Step 8070/10000, loss: 0.04606405273079872\n",
      "Step 8080/10000, loss: 0.12103277444839478\n",
      "Step 8090/10000, loss: 0.04768159240484238\n",
      "Step 8100/10000, loss: 0.05023377388715744\n",
      "Step 8110/10000, loss: 0.03905821591615677\n",
      "Step 8120/10000, loss: 0.15166610479354858\n",
      "Step 8130/10000, loss: 0.09013375639915466\n",
      "Step 8140/10000, loss: 0.0913725197315216\n",
      "Step 8150/10000, loss: 0.05282697081565857\n",
      "Step 8160/10000, loss: 0.09642712771892548\n",
      "Step 8170/10000, loss: 0.07688876986503601\n",
      "Step 8180/10000, loss: 0.039162009954452515\n",
      "Step 8190/10000, loss: 0.06312225013971329\n",
      "Step 8200/10000, loss: 0.05757587403059006\n",
      "Step 8210/10000, loss: 0.047181617468595505\n",
      "Step 8220/10000, loss: 0.11996881663799286\n",
      "Step 8230/10000, loss: 0.020223718136548996\n",
      "Step 8240/10000, loss: 0.03665684536099434\n",
      "Step 8250/10000, loss: 0.06938289105892181\n",
      "Step 8260/10000, loss: 0.04541882127523422\n",
      "Step 8270/10000, loss: 0.06485803425312042\n",
      "Step 8280/10000, loss: 0.08315584063529968\n",
      "Step 8290/10000, loss: 0.04807237163186073\n",
      "Step 8300/10000, loss: 0.13345949351787567\n",
      "Step 8310/10000, loss: 0.06491610407829285\n",
      "Step 8320/10000, loss: 0.06508612632751465\n",
      "Step 8330/10000, loss: 0.05930982530117035\n",
      "Step 8340/10000, loss: 0.04425099492073059\n",
      "Step 8350/10000, loss: 0.08702985942363739\n",
      "Step 8360/10000, loss: 0.07604271173477173\n",
      "Step 8370/10000, loss: 0.08448395878076553\n",
      "Step 8380/10000, loss: 0.12621332705020905\n",
      "Step 8390/10000, loss: 0.1694459170103073\n",
      "Step 8400/10000, loss: 0.06430058926343918\n",
      "Step 8410/10000, loss: 0.05286150425672531\n",
      "Step 8420/10000, loss: 0.05134176090359688\n",
      "Step 8430/10000, loss: 0.029949519783258438\n",
      "Step 8440/10000, loss: 0.1681620478630066\n",
      "Step 8450/10000, loss: 0.05355365574359894\n",
      "Step 8460/10000, loss: 0.02201400138437748\n",
      "Step 8470/10000, loss: 0.17310361564159393\n",
      "Step 8480/10000, loss: 0.05802947282791138\n",
      "Step 8490/10000, loss: 0.025615984573960304\n",
      "Step 8500/10000, loss: 0.08919169008731842\n",
      "Step 8510/10000, loss: 0.12157714366912842\n",
      "Step 8520/10000, loss: 0.07511496543884277\n",
      "Step 8530/10000, loss: 0.12682773172855377\n",
      "Step 8540/10000, loss: 0.12140494585037231\n",
      "Step 8550/10000, loss: 0.05541429668664932\n",
      "Step 8560/10000, loss: 0.058350175619125366\n",
      "Step 8570/10000, loss: 0.06464570760726929\n",
      "Step 8580/10000, loss: 0.03022046387195587\n",
      "Step 8590/10000, loss: 0.06273499876260757\n",
      "Step 8600/10000, loss: 0.059783272445201874\n",
      "Step 8610/10000, loss: 0.10362498462200165\n",
      "Step 8620/10000, loss: 0.05737561732530594\n",
      "Step 8630/10000, loss: 0.036846209317445755\n",
      "Step 8640/10000, loss: 0.08704168349504471\n",
      "Step 8650/10000, loss: 0.06558746844530106\n",
      "Step 8660/10000, loss: 0.07574281841516495\n",
      "Step 8670/10000, loss: 0.028714032843708992\n",
      "Step 8680/10000, loss: 0.05115111917257309\n",
      "Step 8690/10000, loss: 0.05016406998038292\n",
      "Step 8700/10000, loss: 0.07491181790828705\n",
      "Step 8710/10000, loss: 0.02997588738799095\n",
      "Step 8720/10000, loss: 0.057606592774391174\n",
      "Step 8730/10000, loss: 0.1092667505145073\n",
      "Step 8740/10000, loss: 0.04757183417677879\n",
      "Step 8750/10000, loss: 0.08304648846387863\n",
      "Step 8760/10000, loss: 0.02925332635641098\n",
      "Step 8770/10000, loss: 0.10645894706249237\n",
      "Step 8780/10000, loss: 0.1347009539604187\n",
      "Step 8790/10000, loss: 0.045517999678850174\n",
      "Step 8800/10000, loss: 0.05710844695568085\n",
      "Step 8810/10000, loss: 0.08973816782236099\n",
      "Step 8820/10000, loss: 0.1303911656141281\n",
      "Step 8830/10000, loss: 0.05144515633583069\n",
      "Step 8840/10000, loss: 0.048850636929273605\n",
      "Step 8850/10000, loss: 0.09271585941314697\n",
      "Step 8860/10000, loss: 0.05614624172449112\n",
      "Step 8870/10000, loss: 0.04235932603478432\n",
      "Step 8880/10000, loss: 0.1155695915222168\n",
      "Step 8890/10000, loss: 0.020076049491763115\n",
      "Step 8900/10000, loss: 0.03601011633872986\n",
      "Step 8910/10000, loss: 0.03343307226896286\n",
      "Step 8920/10000, loss: 0.06769552826881409\n",
      "Step 8930/10000, loss: 0.1543446183204651\n",
      "Step 8940/10000, loss: 0.1441752016544342\n",
      "Step 8950/10000, loss: 0.06172086298465729\n",
      "Step 8960/10000, loss: 0.1377105563879013\n",
      "Step 8970/10000, loss: 0.07902044802904129\n",
      "Step 8980/10000, loss: 0.048724085092544556\n",
      "Step 8990/10000, loss: 0.09995469450950623\n",
      "Step 9000/10000, loss: 0.0510028675198555\n",
      "Step 9010/10000, loss: 0.02183459885418415\n",
      "Step 9020/10000, loss: 0.02646869793534279\n",
      "Step 9030/10000, loss: 0.0401761457324028\n",
      "Step 9040/10000, loss: 0.10559137165546417\n",
      "Step 9050/10000, loss: 0.06316106021404266\n",
      "Step 9060/10000, loss: 0.07180926948785782\n",
      "Step 9070/10000, loss: 0.07045198976993561\n",
      "Step 9080/10000, loss: 0.05895295366644859\n",
      "Step 9090/10000, loss: 0.02052873745560646\n",
      "Step 9100/10000, loss: 0.045379724353551865\n",
      "Step 9110/10000, loss: 0.032290711998939514\n",
      "Step 9120/10000, loss: 0.04225711151957512\n",
      "Step 9130/10000, loss: 0.08073578774929047\n",
      "Step 9140/10000, loss: 0.10374957323074341\n",
      "Step 9150/10000, loss: 0.016912691295146942\n",
      "Step 9160/10000, loss: 0.07228213548660278\n",
      "Step 9170/10000, loss: 0.05151232331991196\n",
      "Step 9180/10000, loss: 0.04165579378604889\n",
      "Step 9190/10000, loss: 0.06763353198766708\n",
      "Step 9200/10000, loss: 0.027725232765078545\n",
      "Step 9210/10000, loss: 0.12316831946372986\n",
      "Step 9220/10000, loss: 0.04861785098910332\n",
      "Step 9230/10000, loss: 0.03866562992334366\n",
      "Step 9240/10000, loss: 0.04953250661492348\n",
      "Step 9250/10000, loss: 0.07671903073787689\n",
      "Step 9260/10000, loss: 0.023400060832500458\n",
      "Step 9270/10000, loss: 0.06815923750400543\n",
      "Step 9280/10000, loss: 0.05558127909898758\n",
      "Step 9290/10000, loss: 0.07296500355005264\n",
      "Step 9300/10000, loss: 0.08332867920398712\n",
      "Step 9310/10000, loss: 0.024268005043268204\n",
      "Step 9320/10000, loss: 0.07363048195838928\n",
      "Step 9330/10000, loss: 0.022410891950130463\n",
      "Step 9340/10000, loss: 0.05196378752589226\n",
      "Step 9350/10000, loss: 0.06164819747209549\n",
      "Step 9360/10000, loss: 0.0922028049826622\n",
      "Step 9370/10000, loss: 0.053303733468055725\n",
      "Step 9380/10000, loss: 0.11628926545381546\n",
      "Step 9390/10000, loss: 0.042709846049547195\n",
      "Step 9400/10000, loss: 0.031227344647049904\n",
      "Step 9410/10000, loss: 0.05768123269081116\n",
      "Step 9420/10000, loss: 0.17210499942302704\n",
      "Step 9430/10000, loss: 0.037612754851579666\n",
      "Step 9440/10000, loss: 0.057145263999700546\n",
      "Step 9450/10000, loss: 0.04452899470925331\n",
      "Step 9460/10000, loss: 0.028903361409902573\n",
      "Step 9470/10000, loss: 0.10990112274885178\n",
      "Step 9480/10000, loss: 0.05777363479137421\n",
      "Step 9490/10000, loss: 0.061799269169569016\n",
      "Step 9500/10000, loss: 0.03420298174023628\n",
      "Step 9510/10000, loss: 0.07160419970750809\n",
      "Step 9520/10000, loss: 0.03346259146928787\n",
      "Step 9530/10000, loss: 0.057869382202625275\n",
      "Step 9540/10000, loss: 0.026136282831430435\n",
      "Step 9550/10000, loss: 0.13847783207893372\n",
      "Step 9560/10000, loss: 0.11936551332473755\n",
      "Step 9570/10000, loss: 0.07100215554237366\n",
      "Step 9580/10000, loss: 0.04334336146712303\n",
      "Step 9590/10000, loss: 0.04973045364022255\n",
      "Step 9600/10000, loss: 0.031885355710983276\n",
      "Step 9610/10000, loss: 0.06236469745635986\n",
      "Step 9620/10000, loss: 0.07604958117008209\n",
      "Step 9630/10000, loss: 0.03739858791232109\n",
      "Step 9640/10000, loss: 0.06568177789449692\n",
      "Step 9650/10000, loss: 0.03889116272330284\n",
      "Step 9660/10000, loss: 0.060668736696243286\n",
      "Step 9670/10000, loss: 0.09588506817817688\n",
      "Step 9680/10000, loss: 0.019318299368023872\n",
      "Step 9690/10000, loss: 0.1326734721660614\n",
      "Step 9700/10000, loss: 0.037162553519010544\n",
      "Step 9710/10000, loss: 0.015458296053111553\n",
      "Step 9720/10000, loss: 0.09682817757129669\n",
      "Step 9730/10000, loss: 0.06933064758777618\n",
      "Step 9740/10000, loss: 0.04613707214593887\n",
      "Step 9750/10000, loss: 0.02795192040503025\n",
      "Step 9760/10000, loss: 0.05474424362182617\n",
      "Step 9770/10000, loss: 0.04027736186981201\n",
      "Step 9780/10000, loss: 0.07009533792734146\n",
      "Step 9790/10000, loss: 0.0373573899269104\n",
      "Step 9800/10000, loss: 0.0906510204076767\n",
      "Step 9810/10000, loss: 0.060595426708459854\n",
      "Step 9820/10000, loss: 0.09809708595275879\n",
      "Step 9830/10000, loss: 0.05811338499188423\n",
      "Step 9840/10000, loss: 0.09341735392808914\n",
      "Step 9850/10000, loss: 0.10898935794830322\n",
      "Step 9860/10000, loss: 0.09121192246675491\n",
      "Step 9870/10000, loss: 0.07085223495960236\n",
      "Step 9880/10000, loss: 0.0431874543428421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9890/10000, loss: 0.047948725521564484\n",
      "Step 9900/10000, loss: 0.05148473381996155\n",
      "Step 9910/10000, loss: 0.056243542581796646\n",
      "Step 9920/10000, loss: 0.06877101957798004\n",
      "Step 9930/10000, loss: 0.06394119560718536\n",
      "Step 9940/10000, loss: 0.05736568942666054\n",
      "Step 9950/10000, loss: 0.053183428943157196\n",
      "Step 9960/10000, loss: 0.09416409581899643\n",
      "Step 9970/10000, loss: 0.12314668297767639\n",
      "Step 9980/10000, loss: 0.022622298449277878\n",
      "Step 9990/10000, loss: 0.046620361506938934\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    batch_x, batch_y = mnist.train.next_batch(128)\n",
    "    _,loss_value = sess.run([train_op,loss], feed_dict={x: batch_x, y: batch_y})\n",
    "    if i%10 is 0:\n",
    "        print('Step {}/10000, loss: {}'.format(i,loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy after 1000 step: 0.9724000692367554\n"
     ]
    }
   ],
   "source": [
    "acc_score = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "print('Model accuracy after 1000 step: {}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize an image with the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions: [  5.68208520e-07   6.34495382e-06   1.13878332e-05   1.30923057e-04\n",
      "   5.46745548e-04   3.25938913e-07   1.12337606e-08   9.94730592e-01\n",
      "   1.66203808e-05   4.55644121e-03]\n",
      "Class predicted 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAg/kBThB5L6P1ZRSyHs+RAtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0267d02f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "img_batch,pred = sess.run([x,prediction], feed_dict={x:batch_x,y:batch_y})\n",
    "img = img_batch[0]\n",
    "img = np.reshape(img,(28,28))\n",
    "plt.imshow(img, cmap='gray')\n",
    "print('Raw predictions: {}'.format(pred[0]))\n",
    "print('Class predicted {}'.format(np.argmax(pred[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
